import requests
import pandas as pd
import time
import random
from datetime import datetime

def crawl_bunjang_women(keywords: list, pages: int = 5, title_filters: list = None) -> pd.DataFrame:
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                      "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15",
        "Accept": "application/json, text/plain, */*",
        "Origin": "https://m.bunjang.co.kr",
        "Referer": "https://m.bunjang.co.kr/"
    }

    all_items = {}
    
    for keyword in keywords:
        print(f"\n [{keyword}] í¬ë¡¤ë§ ì‹œì‘")
        for page in range(1, pages + 1):
            try:
                resp = requests.get(
                    "https://api.bunjang.co.kr/api/1/find_v2.json",
                    headers=headers,
                    params={
                        "q": keyword,
                        "order": "score",
                        "page": page,
                        "f_category_id": "310"
                    },
                    timeout=5
                )
                resp.raise_for_status()
                data = resp.json()
            except Exception as e:
                print(f"[{keyword}] í˜ì´ì§€ {page} ì—ëŸ¬: {e} â†’ ìŠ¤í‚µ")
                time.sleep(delay)
                continue

            items = data.get("list", [])
            if not items:
                print(f"[{keyword}] í˜ì´ì§€ {page} ê²°ê³¼ ì—†ìŒ â†’ ì¤‘ë‹¨")
                break

            for it in items:
                title = it.get("name", "")
                if title_filters and not any(k.lower() in title.lower() for k in title_filters):
                    continue

                pid = it.get("pid")
                if pid in all_items:
                    continue  # ì¤‘ë³µ ì œê±°

                try:
                    ts = it.get("update_time")
                    date = datetime.fromtimestamp(int(ts)).strftime("%Y-%m-%d %H:%M:%S") if ts else ""
                except:
                    date = ""

                try:
                    det = requests.get(
                        f"https://api.bunjang.co.kr/api/pms/v3/products-detail/{pid}?viewerUid=-1",
                        headers=headers,
                        timeout=5
                    )
                    det.raise_for_status()
                    product = det.json().get("data", {}).get("product", {})
                    if not product:
                        continue

                    categories = product.get("categories", [])
                    category_names = [cat.get("name", "") for cat in categories]
                    full_category_path = " > ".join(category_names)


                    if title_filters and any(k.lower() in ["ì½”ìŠ¤", "cos"] for k in title_filters):
                        bad_categories = {"ì½”ìŠ¤íŠ¬/ì½”ìŠ¤í”„ë ˆ", "í•œë³µ", "ê¸°íƒ€ í…Œë§ˆ/ì´ë²¤íŠ¸"}
                        if any(cat in bad_categories for cat in category_names):
                            continue
                            
                    item = {
                        "pid": product.get("pid"),
                        "ìƒí’ˆ ì´ë¦„": product.get("name", ""),
                        "ê°€ê²©": product.get("price", ""),
                        "ì„¤ëª…": product.get("description", "").replace("\n", " ").strip(),
                        "ìƒí’ˆìƒíƒœ": product.get("condition", ""),
                        "íŒë§¤ìƒíƒœ": product.get("saleStatus", ""),
                        "ì°œ ìˆ˜": product.get("metrics", {}).get("favoriteCount", 0),
                        "ì¡°íšŒìˆ˜": product.get("metrics", {}).get("viewCount", 0),
                        "ë°°ì†¡ë¹„": product.get("trade", {}).get("shippingSpecs", {}).get("DEFAULT", {}).get("fee", 0),
                        "ì‚¬ì§„": ";".join([
                            product.get("imageUrl", "").replace("{cnt}", str(i + 1)).replace("{res}", "600")
                            for i in range(product.get("imageCount", 0))
                        ]),
                        "ì¹´í…Œê³ ë¦¬": full_category_path,
                        "ì§ê±°ë˜ì§€ì—­": product.get("geo", {}).get("address", ""),
                        "ìƒí’ˆíƒœê·¸": ", ".join([
                            kw.get("keyword", "") for kw in product.get("keywords", [])
                        ]),
                        "ê²€ìˆ˜ê°€ëŠ¥": product.get("care", False),
                        "ë§í¬": f"https://m.bunjang.co.kr/products/{pid}",
                        "ì—…ë°ì´íŠ¸ì¼": date
                    }

                    all_items[pid] = item

                except Exception as e:
                    print(f"ğŸ” [ìƒì„¸ ì‹¤íŒ¨] pid={pid}: {e}")
                    continue

            print(f"[{keyword}] í˜ì´ì§€ {page} ì™„ë£Œ â†’ ëˆ„ì  ìˆ˜ì§‘ {len(all_items)}ê°œ")
            time.sleep(random.uniform(0.2, 0.6))

    return pd.DataFrame(list(all_items.values()))

if __name__ == "__main__":
    search_keywords = ["í´ë¡œ"]
    title_filters = ["í´ë¡œ","polo","ë„í”„ë¡œë Œ"]
    pages = 166

    df = crawl_bunjang_women(search_keywords, pages=pages, title_filters=title_filters)
    fname = "bunjang_í´ë¡œ_ì—¬ì„±ì˜ë¥˜_ìµœì¢….csv"
    df.to_csv(fname, index=False, encoding="utf-8-sig")
    print(f"\n ì €ì¥ ì™„ë£Œ: {fname} (ì´ {len(df)}ê°œ ìƒí’ˆ)")